{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ba20963",
   "metadata": {},
   "source": [
    "# Pruebas de Precision, Recall y F1\n",
    "\n",
    "En el siguiente notebook se evalúa el desempeño de un clasificador de texto basado en el algoritmo de Naive Bayes, entrenado con un conjunto de noticias categorizadas.\n",
    "\n",
    "Se implementaron manualmente las métricas de evaluación **Precision**, **Recall** y **F1-Score**\n",
    "\n",
    "El flujo del código es el siguiente:\n",
    "\n",
    "- Se carga el dataset.\n",
    "- Se separan los datos en conjuntos de entrenamiento (80%) y prueba (20%).\n",
    "- Se entrena el clasificador con los textos procesados.\n",
    "- Se realiza la predicción de las categorías para los textos de prueba.\n",
    "- Finalmente, se comparan las predicciones con las categorías reales y se calculan las métricas por clase.\n",
    "\n",
    "Este análisis permite verificar qué tan bien generaliza el modelo ante datos no vistos, y detectar si hay clases que están siendo mal clasificadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a1dad3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'classifier.tokenizer'; 'classifier' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m defaultdict, Counter\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mclassifier\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NaiveBayesClassifier\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mload_data\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m shuffle\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\00 Dev\\news-classifier\\api\\classifier\\classifier.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mclassifier\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocess_text\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mNaiveBayesClassifier\u001b[39;00m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'classifier.tokenizer'; 'classifier' is not a package"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "from classifier import NaiveBayesClassifier\n",
    "from load_data import load_dataset\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eb950c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\00 Dev\\news-classifier\\api\\classifier\\TRAINING-DATA\\BBC News Summary\\News Articles\n"
     ]
    }
   ],
   "source": [
    "ruta = \"TRAINING-DATA/BBC News Summary\"\n",
    "path = Path(\"TRAINING-DATA\") / \"BBC News Summary\" / \"News Articles\"\n",
    "print(path.absolute())\n",
    "\n",
    "data_art = load_dataset(path)\n",
    "\n",
    "path = Path(\"TRAINING-DATA\") / \"BBC News Summary\" / \"Summaries\"\n",
    "data_sum = load_dataset(path)\n",
    "        \n",
    "dataset = data_art + data_sum\n",
    "\n",
    "shuffle(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.8 * len(dataset))\n",
    "train_data = dataset[:split]\n",
    "test_data = dataset[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573578fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NaiveBayesClassifier()\n",
    "clf.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb76d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "for text, true_label in test_data:\n",
    "    pred_label = clf.predict(text)\n",
    "    y_true.append(true_label)\n",
    "    y_pred.append(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b5c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set(y_true + y_pred)\n",
    "tp = Counter()\n",
    "fp = Counter()\n",
    "fn = Counter()\n",
    "\n",
    "for true, pred in zip(y_true, y_pred):\n",
    "    if true == pred:\n",
    "        tp[true] += 1\n",
    "    else:\n",
    "        fp[pred] += 1\n",
    "        fn[true] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3de36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase: business\n",
      "  Precision: 1.00\n",
      "  Recall:    0.96\n",
      "  F1-Score:  0.98\n",
      "\n",
      "Clase: entertainment\n",
      "  Precision: 0.99\n",
      "  Recall:    0.99\n",
      "  F1-Score:  0.99\n",
      "\n",
      "Clase: politics\n",
      "  Precision: 0.96\n",
      "  Recall:    1.00\n",
      "  F1-Score:  0.98\n",
      "\n",
      "Clase: sport\n",
      "  Precision: 1.00\n",
      "  Recall:    1.00\n",
      "  F1-Score:  1.00\n",
      "\n",
      "Clase: tech\n",
      "  Precision: 0.97\n",
      "  Recall:    0.98\n",
      "  F1-Score:  0.97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for label in sorted(labels):\n",
    "    precision = tp[label] / (tp[label] + fp[label]) if (tp[label] + fp[label]) > 0 else 0.0\n",
    "    recall = tp[label] / (tp[label] + fn[label]) if (tp[label] + fn[label]) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    print(f\"Clase: {label}\")\n",
    "    print(f\"  Precision: {precision:.2f}\")\n",
    "    print(f\"  Recall:    {recall:.2f}\")\n",
    "    print(f\"  F1-Score:  {f1:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7582528",
   "metadata": {},
   "source": [
    "## Resultados de Métricas por Clase\n",
    "\n",
    "A continuación se presentan los valores obtenidos para las métricas de **Precisión**, **Recall** y **F1-Score** luego de evaluar el modelo Naive Bayes sobre el conjunto de prueba.\n",
    "\n",
    "| Categoría      | Precisión | Recall | F1-Score |\n",
    "|----------------|-----------|--------|----------|\n",
    "| Business       | 0.98      | 0.97   | 0.98     |\n",
    "| Entertainment  | 0.99      | 0.97   | 0.98     |\n",
    "| Politics       | 0.96      | 0.99   | 0.98     |\n",
    "| Sport          | 1.00      | 0.99   | 0.99     |\n",
    "| Tech           | 0.97      | 0.99   | 0.98     |\n",
    "\n",
    "### Interpretación:\n",
    "\n",
    "- El modelo muestra un desempeño muy alto en todas las categorías, con valores de F1-Score iguales o superiores a **0.98**.\n",
    "- La categoría **Sport** tuvo una precisión perfecta (**1.00**), indicando que no hubo falsos positivos.\n",
    "- En general, hay un buen equilibrio entre precisión y exhaustividad, lo cual demuestra que el modelo logra clasificar correctamente la mayoría de los textos sin sobreajustarse a ninguna clase específica.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
